{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CMPE257_Data_Miners_Final_Exam_All_Sections.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "8-SSksyX2pAQ",
        "J_88Dn4o7IaK",
        "AGRD9mSZ3aec",
        "KwRYiEP4qn6a",
        "3pselFqwqn6h",
        "UcELX95A3_22",
        "5_24ZRQG6hji",
        "8j4Ai7Fc6Gqf"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rameshavinash94/cmpe257_final_exam/blob/main/CMPE257_Data_Miners_Final_Exam_All_Sections.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Team:** Data Miners\n",
        "\n",
        "**Team Members:**\n",
        "1. Avinash Ramesh\n",
        "2. Poojashree NS\n",
        "3. Abraham Kong\n",
        "**Meet with team to discuss what factors you would/should investigate to make an investment decision!! What factors [think about the way you did AlternusVera for factors and microfactors] need to know about the company it's products it's partners and customers and officers ??**\n",
        "\n",
        "As a team we went through couple of \"[articles](https://investinganswers.com/articles/important-factors-before-investing-company)\" to get factors and microfactors that helps to decide weather to invest in a company or not.\n",
        "\n",
        "After discussion, we understanded the requirements and found out few stuff and came up architecture similar to this.\n",
        "\n",
        "![](https://github.com/rameshavinash94/cmpe257_final_exam/blob/main/image%20(1).jpg?raw=true)\n",
        "\n",
        "**here instead of wikipedia, user would type in a company and we amalagamated data from multiple sources.**\n",
        "\n",
        "**API'S**\n",
        "\n",
        "FINANCIALMODELINGPREP - extract the financial details of the company https://site.financialmodelingprep.com/\n",
        "\n",
        "PEOPLESDATA - extract the comapany details from peoplesdata api https://www.peopledatalabs.com/\n",
        "\n",
        "NEWSAPI - extract the news related to company using company name https://newsapi.org/\n",
        "\n",
        "**SCRAPPER:**\n",
        "\n",
        "CRUNCHBASE to extract other import info about the company https://www.crunchbase.com/\n",
        "\n",
        "\n",
        "The below vidoe gave us a broaded picture of what can be done and how to proceed.\n",
        "https://www.youtube.com/watch?v=nYQLp7itZx8 \n",
        "\n",
        "\n",
        "**Find people in the news connected with the company. Find product announcements, partnerships, rounds of funding news + crunchbase**\n",
        "\n",
        "Data to perform the analysis has been scraped from crunchbase and extracted finianial data and company info form the above mentioned api's and get relevant news data from news API. Crunch base also provides the data related people and their relationship with the company, funding, financial model, along with that we have also considered similar companies, technologies and descriptions.\n",
        "We have amalgamated all the data and performed grammer corrections with coreferencing as a preprocessing to graph.\n",
        "\n",
        "**Traversal Algorithm : graph sage as an example.Traverse the graph and deposit the textual info in a text file as the output of the research project**\n",
        "\n",
        "One we have the Amalgamated date, our process goes through various pharases to generate relevant Knowledge graphs.\n",
        "\n",
        "\n",
        "1. PERFROM NLP PREPROCESSING OPERATIONS USING SPACY\n",
        "2. ADD SVO TRIPLETS TO GRAPH\n",
        "3. UPDATE NODE LABES AND PROPERTIES with the help of Google Knowledge graph search api.\n",
        "4. DEDUP AND CLEAN THE GRAPH\n",
        "5. CREATE GRAPH\n",
        "6. GENERATE GRAPH EMEDDING\n",
        "7. If tIme, Permits some ML\n",
        "\n",
        "We used neo4j to store the knowledge graph. Neo4j also has abilities to traverse the graph and display the relationship bewteen nodes in any format.\n",
        "We also stored embeddings of each node as a node property.\n",
        "\n",
        "\n",
        "**Example KE we generated for firm: Coinbase.**\n",
        "\n",
        "\n",
        "<img src=\"https://github.com/rameshavinash94/cmpe257_final_exam/blob/main/graph%20(2).png?raw=true\" width=\"250\"></img>\n",
        "\n",
        "LINK FOR VISUALIZATION: \n",
        "\n",
        "<a href=\"https://9cdc6da62c21d3a11ccc8ec585ee8f47.neo4jsandbox.com/browser/?token=pwfetch:9cdc6da62c21d3a11ccc8ec585ee8f47:eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCIsImtpZCI6IlFUbENPRVV4UmtJNFJETkROakpETXpBME5EZzBRelV3UWpNek9UVTVNRFF4TlRKRk56STJOZyJ9.eyJlbWFpbCI6ImF2aW5hc2gucmFtZXNoQHNqc3UuZWR1IiwiZmFtaWx5X25hbWUiOiJSYW1lc2giLCJnaXZlbl9uYW1lIjoiQXZpbmFzaCIsImxvY2FsZSI6ImVuIiwibmFtZSI6IkF2aW5hc2ggUmFtZXNoIiwibmlja25hbWUiOiJhdmluYXNoLnJhbWVzaCIsInBpY3R1cmUiOiJodHRwczovL2xoMy5nb29nbGV1c2VyY29udGVudC5jb20vYS9BQVRYQUp3SXNjMDFPQWtzcFRiTnVZM1A5bHBHMkl2RzktY3VGMHVhVzV3PXM5Ni1jIiwidXNlcl9tZXRhZGF0YSI6eyJjb21wYW55IjoiU3R1ZGVudCJ9LCJhcHBfbWV0YWRhdGEiOnsic2FuZGJveHYzIjp7ImNyZWF0ZWRBdCI6MTY1MzMzNzQ1ODIyMiwiYWdyZWVkVG9UZXJtc0F0IjoxNjUzMzM3NDY2NzQ0fX0sInNhbmRib3h2MyI6eyJjcmVhdGVkQXQiOjE2NTMzMzc0NTgyMjIsImFncmVlZFRvVGVybXNBdCI6MTY1MzMzNzQ2Njc0NH0sImZpcmViYXNlX2RhdGEiOnsidWlkIjoiZ29vZ2xlLW9hdXRoMnwxMTgxODc1NDA3MDUxMzcwMzY0MzcifSwic2NvcGVzIjp7InNhbmRib3hlcyI6WyJzYm94MSIsInNib3gyIiwic2JveDMiXX0sImNsaWVudElEIjoiRHhobWlGOFRDZXpuSTdYb2kwOFV5WVNjTEdabms0a2UiLCJjcmVhdGVkX2F0IjoiMjAyMi0wNS0yM1QyMDoyNDoxNi4zNDVaIiwiZW1haWxfdmVyaWZpZWQiOnRydWUsImlkZW50aXRpZXMiOlt7InByb3ZpZGVyIjoiZ29vZ2xlLW9hdXRoMiIsInVzZXJfaWQiOiIxMTgxODc1NDA3MDUxMzcwMzY0MzciLCJjb25uZWN0aW9uIjoiZ29vZ2xlLW9hdXRoMiIsImlzU29jaWFsIjp0cnVlfV0sInVwZGF0ZWRfYXQiOiIyMDIyLTA1LTIzVDIwOjI0OjMyLjYyMVoiLCJ1c2VyX2lkIjoiZ29vZ2xlLW9hdXRoMnwxMTgxODc1NDA3MDUxMzcwMzY0MzciLCJpc3MiOiJodHRwczovL2xvZ2luLm5lbzRqLmNvbS8iLCJzdWIiOiJnb29nbGUtb2F1dGgyfDExODE4NzU0MDcwNTEzNzAzNjQzNyIsImF1ZCI6IkR4aG1pRjhUQ2V6bkk3WG9pMDhVeVlTY0xHWm5rNGtlIiwiaWF0IjoxNjUzMzM5MjA1LCJleHAiOjE2NTM0MjU2MDUsIm5vbmNlIjoiWm01blVISTVZVlp1VWt0d1IyTjFVbVJSYjNWcVExcEtMbEpqUzJsalRscFlkRmxvUW5Wb2IzWmFUZz09In0.fy82GKMn19MU2yZTXZfTVjvP2dxLVMTtsBwsWy4nDriQnF5YBNh3OQEWgOKjvbQEHsKD_04z_gTS0PLabMjbXm7Au0XpkLqlgKOBxJgjil5FtkgJmnu--2EM5gxmZD32RblEiwRrf-qHG9ToLvxxVZgtJwuUHB7PMKMxzEIJPgLc60drCH-lMDbBOILp_Q1CGtO36z-Cp0k9HkZffCLehdXuN7ac155XBFnLltzf3KG9O14jscnFLchsXocJObRlv-8aXAaA8iF-bP38-S_9oUdXQ4gh2sVgzkQbFoodt1ozgmiat5yGsL36hr2SKchMfi-75g6G_9Nt-qlN5PdLUw\"> Click here to visualize the graph in Neo4j Browser</a>\n",
        "\n",
        "RUN THE BELOW CYPHER QUERY to GET GRAPH OUTPUTS\n",
        "\n",
        "```MATCH (n) RETURN n```\n",
        "\n",
        "We used Cypher a declarative graph query language that allows for expressive and efficient data querying in a property graph.\n",
        "\n",
        "**The format of the output should be easily consumable by a HUMAN!!**\n",
        "\n",
        "For human understandable format we are summarizing the data obtained from graph and displaying it. So the end user can read the summary about the company and decide where to invest or not.\n",
        "Also, the final source, target and relationship of nodes are traversed from KE and finally stored in text file.\n",
        "\n",
        "***We have also provided the option for the end user to provide the company name that they are interested in investing and builts knowledge graph dynamically and provides him with all important info and summary about the company.***\n",
        "\n",
        "**KNOWN ISSUES AND SOLUTION:**\n",
        "\n",
        "1. Importing Spacy needs a restart of runtime after installaiton.\n",
        "2. CRUNCHBASE SCRAPPING MAY NOT WORK SOMETIMES AND SOME DATA MAY BE LOST DURING DATA AMALGAMATION\n",
        "3. NOTE OUR GRAPH DB CONNECTION WILL BE LOST IN ANOTHER 3 DAYS BECAUSE OF FREE TRIAL"
      ],
      "metadata": {
        "id": "dCf2_drCLCOU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **SECTION -1 COMPANY DATA EXTRACTION AND AMALGAMATION FOR KNOWLEDGE GRAPH BUILDING**"
      ],
      "metadata": {
        "id": "KqeFBaSS6sYp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**DATA SOURCES**\n",
        "\n",
        "**API'S**\n",
        "\n",
        "*FINANCIALMODELINGPREP* - extract the financial details of the company\n",
        "*https://site.financialmodelingprep.com/*\n",
        "\n",
        "*PEOPLESDATA*  - extract the comapany details from peoplesdata api\n",
        "*https://www.peopledatalabs.com/*\n",
        "\n",
        "*NEWSAPI* - extract the news related to company using company name \n",
        "*https://newsapi.org/*\n",
        "\n",
        "\n",
        "**SCRAPPER:**\n",
        "\n",
        "*CRUNCHBASE* to extract other import info about the company\n",
        "*https://www.crunchbase.com/*"
      ],
      "metadata": {
        "id": "U8Fbr_tfwfMS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **FINAL SCRAPPER/EXTRACTOR CLASS TO GET DATA FROM ALL SOURCES**"
      ],
      "metadata": {
        "id": "8-SSksyX2pAQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "\n",
        "class Company_info_Scrapper:\n",
        "\n",
        "  def __init__(self,name):\n",
        "    self.name=name\n",
        "\n",
        "  def financialmodelingprep(self,ticker):\n",
        "    company_ticker_name=ticker\n",
        "    url = (\"https://financialmodelingprep.com/api/v3/profile/{company_ticker_name}?apikey=313947d8a8586e2c95e019557dc4f8ce\").format(company_ticker_name=company_ticker_name)\n",
        "    response = requests.get(url)\n",
        "    data = dict(response.json()[0])\n",
        "    financial_data=''\n",
        "    for x,y in data.items():\n",
        "      info='{x} is {y}.'.format(x=x,y=y)\n",
        "      financial_data+=info\n",
        "    return financial_data\n",
        "\n",
        "  def peoples_data_func(self,company):\n",
        "    url = \"https://api.peopledatalabs.com/v5/company/clean?name={company}&pretty=true\".format(company=company)\n",
        "    headers = {\n",
        "        \"Accept\": \"application/json\",\n",
        "        \"Content-Type\": \"application/json\",\n",
        "        \"X-API-Key\": \"867056d7d2fc1e150e9bd1311d870526d75070f45edf045331edf8d7bb72d097\"\n",
        "    }\n",
        "    PEOPLESDATA = requests.get(url, headers=headers)\n",
        "    json_object = json.dumps(PEOPLESDATA.json())\n",
        "    with open(\"people_data.json\", \"w\") as outfile:\n",
        "      outfile.write(json_object)\n",
        "    peoples_dict= dict(json.load(open('people_data.json')))\n",
        "    peoples_data=''\n",
        "    finance_data=''\n",
        "    for x,y in peoples_dict.items():\n",
        "      if x=='ticker' and y!=None:\n",
        "        finance_data = self.financialmodelingprep(y)\n",
        "      if x!='status' and y!=None and x!='location':\n",
        "        info='{x} is {y}.'.format(x=x,y=y)\n",
        "        peoples_data+=info\n",
        "    if x=='location':\n",
        "      for z,c in y.items():\n",
        "        info='{z} is {c}.'.format(z=z,c=c)\n",
        "        peoples_data+=info\n",
        "    if finance_data!='':\n",
        "      return peoples_data,finance_data\n",
        "    else:\n",
        "      return peoples_data\n",
        "\n",
        "  def fetch_news(self,company):\n",
        "    url = ('https://newsapi.org/v2/everything?'\n",
        "        'q={company}&'\n",
        "        'sortBy=popularity&'\n",
        "        'apiKey=78f8b5ac018e4c568badb6877ac4f778').format(company=company)\n",
        "    response = requests.get(url)\n",
        "    news_data = response.json()\n",
        "    json_object = json.dumps(news_data)\n",
        "    with open(\"news.json\", \"w\") as outfile:\n",
        "      outfile.write(json_object) \n",
        "    df = pd.read_json('/content/news.json')\n",
        "    news_data = pd.json_normalize(news_data,record_path=['articles']).iloc[:10,:]\n",
        "    data = news_data['title'] + '.' + news_data['description']\n",
        "    news_info = str('\\n'.join(data.values))\n",
        "    return news_info\n",
        "  \n",
        "  def crunchbase_board(self,company):\n",
        "    board_members=\"\"\n",
        "    url='https://www.crunchbase.com/organization/{company}/people'.format(company=company)\n",
        "    headers = {\"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.14; rv:66.0) Gecko/20100101 Firefox/66.0\", \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\", \"Accept-Language\": \"en-US,en;q=0.5\", \"Accept-Encoding\": \"gzip, deflate\", \"DNT\": \"1\", \"Connection\": \"close\", \"Upgrade-Insecure-Requests\": \"1\"}\n",
        "    html_content = requests.get(url,headers=headers).text\n",
        "    doc=BeautifulSoup(html_content,'html.parser')\n",
        "    for div in doc.find_all('div',attrs={\"class\" :\"fields\"}):\n",
        "      test=div.text\n",
        "      board_members+=str(test)+\".\"  \n",
        "    return board_members\n",
        "\n",
        "  def crunchbase_funding(self,company):\n",
        "    funding_data =\"\"\n",
        "    url='https://www.crunchbase.com/organization/{company}/company_financials'.format(company=company)\n",
        "    headers = {\"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.14; rv:66.0) Gecko/20100101 Firefox/66.0\", \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\", \"Accept-Language\": \"en-US,en;q=0.5\", \"Accept-Encoding\": \"gzip, deflate\", \"DNT\": \"1\", \"Connection\": \"close\", \"Upgrade-Insecure-Requests\": \"1\"}\n",
        "    html_content = requests.get(url,headers=headers).text\n",
        "    doc=BeautifulSoup(html_content,'html.parser')\n",
        "    for div in doc.find_all('a',attrs={\"class\" :\"link-primary\"}):\n",
        "      test=div.text.replace(u'\\xa0', u' is ')\n",
        "      funding_data+=str(test)+\".\"\n",
        "    return funding_data\n",
        "  \n",
        "  def crunchbase_company_description(self,company):\n",
        "    company_description=\"\"\n",
        "    url='https://www.crunchbase.com/organization/{company}'.format(company=company)\n",
        "    headers = {\"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.14; rv:66.0) Gecko/20100101 Firefox/66.0\", \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\", \"Accept-Language\": \"en-US,en;q=0.5\", \"Accept-Encoding\": \"gzip, deflate\", \"DNT\": \"1\", \"Connection\": \"close\", \"Upgrade-Insecure-Requests\": \"1\"}\n",
        "    html_content = requests.get(url,headers=headers).text\n",
        "    doc=BeautifulSoup(html_content,'html.parser')\n",
        "    for div in doc.find_all('span',attrs={\"class\" :\"description\"}):\n",
        "      company_description+=div.text\n",
        "    return company_description\n",
        "\n",
        "  def crunchbase_technology(self,company):\n",
        "    technology_data =\"\"\n",
        "    url='https://www.crunchbase.com/organization/{company}/technology'.format(company=company)\n",
        "    headers = {\"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.14; rv:66.0) Gecko/20100101 Firefox/66.0\", \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\", \"Accept-Language\": \"en-US,en;q=0.5\", \"Accept-Encoding\": \"gzip, deflate\", \"DNT\": \"1\", \"Connection\": \"close\", \"Upgrade-Insecure-Requests\": \"1\"}\n",
        "    html_content = requests.get(url,headers=headers).text\n",
        "    doc=BeautifulSoup(html_content,'html.parser')\n",
        "    for div in doc.find_all('a',attrs={\"class\" :re.compile(\"^link-primary$\")}):\n",
        "      if len(div['class'])==1:\n",
        "        test=div.text.replace(u'\\xa0', u' is ')\n",
        "        technology_data+=str(test)+\".\"\n",
        "    return technology_data\n",
        "\n",
        "  def crunchbase_news_signal(self,company):\n",
        "    news_signal_data =\"\"\n",
        "    url='https://www.crunchbase.com/organization/{company}/signals_and_news'.format(company=company)\n",
        "    headers = {\"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.14; rv:66.0) Gecko/20100101 Firefox/66.0\", \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\", \"Accept-Language\": \"en-US,en;q=0.5\", \"Accept-Encoding\": \"gzip, deflate\", \"DNT\": \"1\", \"Connection\": \"close\", \"Upgrade-Insecure-Requests\": \"1\"}\n",
        "    html_content = requests.get(url,headers=headers).text\n",
        "    doc=BeautifulSoup(html_content,'html.parser')\n",
        "    for div in doc.find_all('a',attrs={\"class\" :\"activity-url-title link-accent\"}):\n",
        "      test=div.text.replace(u'\\xa0', u' is ')\n",
        "      news_signal_data+=str(test)+\".\"\n",
        "    return news_signal_data\n",
        "  \n",
        "  def cruncbase_similar_companies(self,company):\n",
        "    similar_companies =\"\"\n",
        "    url='https://www.crunchbase.com/organization/{company}/org_similarity_overview'.format(company=company)\n",
        "    headers = {\"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.14; rv:66.0) Gecko/20100101 Firefox/66.0\", \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\", \"Accept-Language\": \"en-US,en;q=0.5\", \"Accept-Encoding\": \"gzip, deflate\", \"DNT\": \"1\", \"Connection\": \"close\", \"Upgrade-Insecure-Requests\": \"1\"}\n",
        "    html_content = requests.get(url,headers=headers).text\n",
        "    doc=BeautifulSoup(html_content,'html.parser')\n",
        "    for div in doc.find_all('phrase-list-card',attrs={\"class\" :\"ng-star-inserted\"}):\n",
        "      test=div.text.replace(u'\\xa0', u'')\n",
        "      similar_companies+=str(test)\n",
        "    return similar_companies\n",
        "\n",
        "  def Consolidated_comapny_info_Scraper(self):\n",
        "    Company_name=self.name\n",
        "    company_description=self.crunchbase_company_description(Company_name)\n",
        "    board_members=self.crunchbase_board(Company_name)\n",
        "    funding_data=self.crunchbase_funding(Company_name)\n",
        "    technology_data=self.crunchbase_technology(Company_name)\n",
        "    news_signal_data=self.crunchbase_news_signal(Company_name)\n",
        "    similar_companies=self.cruncbase_similar_companies(Company_name)\n",
        "    news_info=self.fetch_news(Company_name)\n",
        "    peoples_data_values = self.peoples_data_func(Company_name)\n",
        "    if (type(peoples_data_values) == type((\"apple\", \"banana\"))):\n",
        "      peoples_data, financial_data = peoples_data_values\n",
        "    else:\n",
        "      peoples_data = peoples_data_values\n",
        "      financial_data=''\n",
        "    amalgamated_data = Company_name+\":\"+\"\\n\"+company_description+\"\\n\"+board_members+\"\\n\"+funding_data+\"\\n\"+technology_data+\"\\n\"+news_signal_data+\"\\n\"+similar_companies+\"\\n\"+news_info+\"\\n\"+peoples_data+\"\\n\"+financial_data\n",
        "    return amalgamated_data"
      ],
      "metadata": {
        "id": "1vOm7nrpxIL4"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **PASS USER INPUT TO EXTRACT THE NECESSARY DETAILS FOR A PARTICULAR COMPANY**"
      ],
      "metadata": {
        "id": "J_88Dn4o7IaK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "company_info=''\n",
        "Get_user_input = input(\"Enter the company name to extract info!!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "prAFeW1H7S8_",
        "outputId": "5f7286f4-8306-45a5-8782-529a7a03d5a4"
      },
      "execution_count": 36,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter the company name to extract info!!Quillbot\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CREATE OBJECT FOR THE CLASS AND DATA EXTRACTION PROCESS**"
      ],
      "metadata": {
        "id": "Z0MdTeUd3HR4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "extacted_company_data = Company_info_Scrapper(Get_user_input)"
      ],
      "metadata": {
        "id": "qSzGo0EX3GgG"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#extract all the data about the company from various sources\n",
        "company_info = extacted_company_data.Consolidated_comapny_info_Scraper()"
      ],
      "metadata": {
        "id": "LvObJYZl3qPB"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(company_info)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jMdEMll23_u6",
        "outputId": "b7240209-fe48-4b7c-eb86-07f09f731d3c"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Quillbot:\n",
            "\n",
            "\n",
            "support@crunchbase.com.\n",
            "support@crunchbase.com.\n",
            "\n",
            "\n",
            "6 Examples Of Marketing Tools Using Artificial Intelligence (AI).Artificial intelligence (AI) is quickly becoming one of the most popular marketing buzzwords. And for good reason – AI can help us automate repetitive tasks, personalize marketing efforts, and make better decisions, faster! When it comes to increasing brand v…\n",
            "Quillbot Review.Do you want to write a better version of your online content? Quillbot is a paraphrasing tool that switches your text to make it unique and better—almost like a thesaurus for complete sentences! As a bestselling author and content creator, I love programs lik…\n",
            "Best Online Spellchecker – Free and Paid Options.Have you experienced publishing a blog post only to find out there’s a typo in the introduction? Then you should find a program that corrects these writing errors.  Whether you write for business or academic purposes, checking your spelling and grammar is alw…\n",
            "Best Free and Paid Plagiarism Checkers.Running your work on a plagiarism checker to look for duplicate content is essential before publishing it online. Otherwise, you can get sued for intellectual fraud or theft.  I carefully selected the best plagiarism checker for students, professional writers…\n",
            "New Developments At SuRo Capital.This is an update to a previous SuRo Capital article I published in December 2020. Read more to see the fund performance, recent distribution, and portfolio holdings.\n",
            "SuRo Capital's (SSSS) CEO Mark Klein on Q1 2022 Results - Earnings Call Transcript.SuRo Capital Corp. (NASDAQ:NASDAQ:SSSS) Q1 2022 Earnings Conference Call May 4, 2022 5:00 PM ETCompany ParticipantsWilly Lee – Investor RelationsMark Klein – Chairman and Chief Executive...\n",
            "7 AI-Powered Content Creation Tools for Social Media Managers.AI-powered content creation tools can't replace great writers — but they help writers and marketers save time and use their skills for more strategic aspects of content creation.\n",
            "Use a Paraphrase Generator and Reword Your Marketing Copy.Coming up with New Variations for your Marketing Copy is hard! Let AI do the heavy lifting. Use a Paraphrase Generator and Reword Your Marketing Copy.\n",
            "name is quillbot.size is 11-50.id is quillbot.industry is computer software.linkedin_url is linkedin.com/company/quillbot.linkedin_id is 14811938.website is quillbot.com.type is private.raw is ['quillbot'].score is 1.0.fuzzy_match is True.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **TEXT CORRECTION ON THE RETREIVED DATA**"
      ],
      "metadata": {
        "id": "AGRD9mSZ3aec"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install language_tool_python"
      ],
      "metadata": {
        "id": "yQ3438gK3aed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2f90d59-780a-406b-b785-07980c2777ad"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: language_tool_python in /usr/local/lib/python3.7/dist-packages (2.7.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from language_tool_python) (4.64.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from language_tool_python) (2.23.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->language_tool_python) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->language_tool_python) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->language_tool_python) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->language_tool_python) (2022.5.18.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import language_tool_python\n",
        "tool = language_tool_python.LanguageTool('en-US')\n",
        "text = company_info\n",
        "correct_company_info = tool.correct(text)"
      ],
      "metadata": {
        "id": "F6HR5bbe3aed"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "correct_company_info = correct_company_info.replace(u'\\xa0', u'')"
      ],
      "metadata": {
        "id": "oD4JfqR33aed"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(correct_company_info)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81eac053-917c-4fd9-988c-fba65b3f8ff2",
        "id": "2m3XvpSZ3aed"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Quillbot:\n",
            "\n",
            "\n",
            "support@crunchbase.com.\n",
            "support@crunchbase.com.\n",
            "\n",
            "\n",
            "6 Examples Of Marketing Tools Using Artificial Intelligence (AI). Artificial intelligence (AI) is quickly becoming one of the most popular marketing buzzwords. And for good reason – AI can help us automate repetitive tasks, personalize marketing efforts, and make better decisions, faster! When it comes to increasing brand v…\n",
            "Quillbot Review. Do you want to write a better version of your online content? Quillbot is a paraphrasing tool that switches your text to make it unique and better—almost like a thesaurus for complete sentences! As a bestselling author and content creator, I love programs like…\n",
            "Best Online Spellchecker – Free and Paid Options. Have you experienced publishing a blog post only to find out there’s a typo in the introduction? Then you should find a program that corrects these writing errors. Whether you write for business or academic purposes, checking your spelling and grammar is ALW…\n",
            "Best Free and Paid Plagiarism Checkers. Running your work on a plagiarism checker to look for duplicate content is essential before publishing it online. Otherwise, you can get sued for intellectual fraud or theft. I carefully selected the best plagiarism checker for students, professional writers…\n",
            "New Developments At Sure Capital. This is an update to a previous Sure Capital article I published in December 2020. Read more to see the fund performance, recent distribution, and portfolio holdings.\n",
            "Sure Capital's (SSSS) CEO Mark Klein on Q1 2022 Results - Earnings Call Transcript. SuRo Capital Corp. (NASDAQ:NASDAQ:SASS) Q1 2022 Earnings Conference Call May 4, 2022 5:00 PM Company Participants Willy Lee – Investor Relations Mark Klein – Chairman and Chief Executive...\n",
            "7 AI-Powered Content Creation Tools for Social Media Managers.AI-powered content creation tools can't replace great writers — but they help writers and marketers save time and use their skills for more strategic aspects of content creation.\n",
            "Use a Paraphrase Generator and Reword Your Marketing Copy. Coming up with New Variations for your Marketing Copy is hard! Let AI do the heavy lifting. Use a Paraphrase Generator and Reword Your Marketing Copy.\n",
            "Name is Quillbot. Size is 11-50.id is Quillbot. Industry is computer software.LinkedIn_URL is linkedin.com/company/quillbot.linkedin_id is 14811938.website is quillbot.com.type is private. Raw is ['Quillbot'].score is 1.0.fuzzy_match is True.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "correct_company_info = correct_company_info.replace('\\n', '')\n",
        "\n",
        "with open('companyInfo.txt', 'w') as f:\n",
        "    f.write(correct_company_info)"
      ],
      "metadata": {
        "id": "8naUBY2B7iIf"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **SECTION -2 KNOWLEDGE_GRAPH BUILDING AND VISUALIZATION**"
      ],
      "metadata": {
        "id": "obumxeS2fwGf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **PERFORM NLP PREPROCESSING OPERATIONS AND BUILDING GRAPH**\n",
        "1. PERFROM NLP PREPROCESSING OPERATIONS USING SPACY\n",
        "2. ADD SVO TRIPLETS TO GRAPH\n",
        "3. UPDATE NODE LABES AND PROPERTIES\n",
        "4. DEDUP AND CLEAN THE GRAPH\n",
        "5. CREATE GRAPH\n",
        "6. GENERATE GRAPH EMEDDING\n",
        "7. If tIme, Permits some ML\n",
        "\n",
        "\n",
        "**GRAPH DB USED: NEO4J**"
      ],
      "metadata": {
        "id": "KwRYiEP4qn6a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**INSTALLING LIBRARIES**"
      ],
      "metadata": {
        "id": "jpP_xSTN3wME"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "source": [
        "!pip install spacy==3.0.3\n",
        "!pip install py2neo\n",
        "!python -m spacy download en_core_web_md"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: spacy==3.0.3 in /usr/local/lib/python3.7/dist-packages (3.0.3)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.0.3) (3.0.9)\n",
            "Requirement already satisfied: pathy in /usr/local/lib/python3.7/dist-packages (from spacy==3.0.3) (0.6.1)\n",
            "Requirement already satisfied: typer<0.4.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.0.3) (0.3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy==3.0.3) (2.11.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy==3.0.3) (57.4.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy==3.0.3) (2.0.6)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy==3.0.3) (0.9.1)\n",
            "Requirement already satisfied: pydantic<1.8.0,>=1.7.1 in /usr/local/lib/python3.7/dist-packages (from spacy==3.0.3) (1.7.4)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy==3.0.3) (3.0.6)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.0.3) (21.3)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.0.3) (8.0.16)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from spacy==3.0.3) (2.0.7)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy==3.0.3) (3.10.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.0.3) (2.23.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.0.3) (4.64.0)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.0.3) (2.4.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.0.3) (1.0.7)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from spacy==3.0.3) (4.11.3)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.0.3) (1.21.6)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.0.3) (0.4.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.1->spacy==3.0.3) (3.8.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy==3.0.3) (3.0.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.0.3) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.0.3) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.0.3) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.0.3) (2022.5.18.1)\n",
            "Requirement already satisfied: click<7.2.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.4.0,>=0.3.0->spacy==3.0.3) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy==3.0.3) (2.0.1)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy->spacy==3.0.3) (5.2.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: py2neo in /usr/local/lib/python3.7/dist-packages (2021.2.3)\n",
            "Requirement already satisfied: interchange~=2021.0.4 in /usr/local/lib/python3.7/dist-packages (from py2neo) (2021.0.4)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from py2neo) (2022.5.18.1)\n",
            "Requirement already satisfied: monotonic in /usr/local/lib/python3.7/dist-packages (from py2neo) (1.6)\n",
            "Requirement already satisfied: pygments>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from py2neo) (2.6.1)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from py2neo) (1.24.3)\n",
            "Requirement already satisfied: pansi>=2020.7.3 in /usr/local/lib/python3.7/dist-packages (from py2neo) (2020.7.3)\n",
            "Requirement already satisfied: six>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from py2neo) (1.15.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from py2neo) (21.3)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from interchange~=2021.0.4->py2neo) (2022.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->py2neo) (3.0.9)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting en-core-web-md==3.0.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.0.0/en_core_web_md-3.0.0-py3-none-any.whl (47.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 47.1 MB 55.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.1.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from en-core-web-md==3.0.0) (3.0.3)\n",
            "Requirement already satisfied: typer<0.4.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (0.3.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (2.0.6)\n",
            "Requirement already satisfied: pathy in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (0.6.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (57.4.0)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (0.4.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (3.0.6)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (2.4.3)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (1.21.6)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (2.0.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (21.3)\n",
            "Requirement already satisfied: pydantic<1.8.0,>=1.7.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (1.7.4)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (4.64.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (0.9.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (3.10.0.2)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (1.0.7)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (3.0.9)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (4.11.3)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (8.0.16)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (2.11.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.1->spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (3.8.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (3.0.9)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (2022.5.18.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (1.24.3)\n",
            "Requirement already satisfied: click<7.2.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.4.0,>=0.3.0->spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (2.0.1)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy->spacy<3.1.0,>=3.0.0->en-core-web-md==3.0.0) (5.2.1)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_md')\n"
          ]
        }
      ],
      "metadata": {
        "id": "0ke8drkMqn6c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb13a23f-e00d-4412-faf9-ee34c9c39608"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**IMPORTING MODULES**"
      ],
      "metadata": {
        "id": "8OMag0fH3t09"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "source": [
        "#required libraries\n",
        "import json\n",
        "import re\n",
        "import urllib\n",
        "from pprint import pprint\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "from py2neo import Node, Graph, Relationship, NodeMatcher\n",
        "from py2neo.bulk import merge_nodes\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "import spacy\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "from spacy.matcher import Matcher\n",
        "from spacy.tokens import Doc, Span, Token\n",
        "\n",
        "print(spacy.__version__)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.0.3\n"
          ]
        }
      ],
      "metadata": {
        "id": "oLtVA0ZHqn6d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "131ccc5f-c66e-401d-92a4-0a67f5ea8758"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "source": [
        "#prerequisites/ properties\n",
        "SUBJECTS = [\"nsubj\", \"nsubjpass\", \"csubj\", \"csubjpass\", \"agent\", \"expl\"]\n",
        "VERBS = ['ROOT', 'advcl']\n",
        "OBJECTS = [\"dobj\", \"dative\", \"attr\", \"oprd\", 'pobj']\n",
        "ENTITY_LABELS = ['PERSON', 'NORP', 'GPE', 'ORG', 'FAC', 'LOC', 'PRODUCT', 'EVENT', 'WORK_OF_ART']\n",
        "#my google knowledge graph api\n",
        "api_key = \"AIzaSyCJ9iPRtK8eIhw0KFBzEhBaXnMTMqFgTSc\"\n",
        "\n",
        "non_nc = spacy.load('en_core_web_md')\n",
        "nlp = spacy.load('en_core_web_md')\n",
        "nlp.add_pipe('merge_noun_chunks')"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function spacy.pipeline.functions.merge_noun_chunks>"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ],
      "metadata": {
        "id": "gxfX7q_nqn6e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "073d22c7-c646-4b40-f9a6-c1f0f048d995"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ALL FUNCTIONS**"
      ],
      "metadata": {
        "id": "h-nq-kCr3rvv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "source": [
        "def query_google(query, api_key, limit=10, indent=True, return_lists=True):\n",
        "    text_ls = []\n",
        "    node_label_ls = []\n",
        "    url_ls = []  \n",
        "    params = {\n",
        "        'query': query,\n",
        "        'limit': limit,\n",
        "        'indent': indent,\n",
        "        'key': api_key,\n",
        "    }      \n",
        "    service_url = 'https://kgsearch.googleapis.com/v1/entities:search'\n",
        "    url = service_url + '?' + urllib.parse.urlencode(params)\n",
        "    response = json.loads(urllib.request.urlopen(url).read()) \n",
        "    if return_lists:\n",
        "        for element in response['itemListElement']:\n",
        "\n",
        "            try:\n",
        "                node_label_ls.append(element['result']['@type'])\n",
        "            except:\n",
        "                node_label_ls.append('')\n",
        "\n",
        "            try:\n",
        "                text_ls.append(element['result']['detailedDescription']['articleBody'])\n",
        "            except:\n",
        "                text_ls.append('')\n",
        "                \n",
        "            try:\n",
        "                url_ls.append(element['result']['detailedDescription']['url'])\n",
        "            except:\n",
        "                url_ls.append('')\n",
        "                \n",
        "        return text_ls, node_label_ls, url_ls\n",
        "    \n",
        "    else:\n",
        "        return response\n",
        "\n",
        "def remove_special_characters(text):\n",
        "    regex = re.compile(r'[\\n\\r\\t]')\n",
        "    clean_text = regex.sub(\" \", text)    \n",
        "    return clean_text\n",
        "\n",
        "\n",
        "def remove_stop_words_and_punct(text, print_text=False): \n",
        "    result_ls = []\n",
        "    rsw_doc = non_nc(text)  \n",
        "    for token in rsw_doc:\n",
        "        if print_text:\n",
        "            print(token, token.is_stop)\n",
        "            print('--------------')\n",
        "        if not token.is_stop and not token.is_punct:\n",
        "            result_ls.append(str(token)) \n",
        "    result_str = ' '.join(result_ls)\n",
        "    return result_str\n",
        "\n",
        "\n",
        "def create_svo_lists(doc, print_lists):    \n",
        "    subject_ls = []\n",
        "    verb_ls = []\n",
        "    object_ls = []\n",
        "    for token in doc:\n",
        "        if token.dep_ in SUBJECTS:\n",
        "            subject_ls.append((token.lower_, token.idx))\n",
        "        elif token.dep_ in VERBS:\n",
        "            verb_ls.append((token.lemma_, token.idx))\n",
        "        elif token.dep_ in OBJECTS:\n",
        "            object_ls.append((token.lower_, token.idx))\n",
        "    if print_lists:\n",
        "        print('SUBJECTS: ', subject_ls)\n",
        "        print('VERBS: ', verb_ls)\n",
        "        print('OBJECTS: ', object_ls)\n",
        "    return subject_ls, verb_ls, object_ls\n",
        "\n",
        "\n",
        "def remove_duplicates(tup, tup_posn):\n",
        "    check_val = set()\n",
        "    result = []  \n",
        "    for i in tup:\n",
        "        if i[tup_posn] not in check_val:\n",
        "            result.append(i)\n",
        "            check_val.add(i[tup_posn])            \n",
        "    return result\n",
        "\n",
        "\n",
        "def remove_dates(tup_ls):\n",
        "    clean_tup_ls = []\n",
        "    for entry in tup_ls:\n",
        "        if not entry[2].isdigit():\n",
        "            clean_tup_ls.append(entry)\n",
        "    return clean_tup_ls\n",
        "\n",
        "\n",
        "def create_svo_triples(text, print_lists=False):    \n",
        "    clean_text = remove_special_characters(text)\n",
        "    doc = nlp(clean_text)\n",
        "    subject_ls, verb_ls, object_ls = create_svo_lists(doc, print_lists=print_lists)\n",
        "    graph_tup_ls = []\n",
        "    dedup_tup_ls = []\n",
        "    clean_tup_ls = []\n",
        "    \n",
        "    for subj in subject_ls: \n",
        "        for obj in object_ls:           \n",
        "            dist_ls = []\n",
        "            for v in verb_ls:             \n",
        "                # Assemble a list of distances between each object and each verb\n",
        "                dist_ls.append(abs(obj[1] - v[1]))             \n",
        "            # Get the index of the verb with the smallest distance to the object \n",
        "            # and return that verb\n",
        "            index_min = min(range(len(dist_ls)), key=dist_ls.__getitem__)       \n",
        "            # Remve stop words from subjects and object.  Note that we do this a bit\n",
        "            # later down in the process to allow for proper sentence recognition.\n",
        "            no_sw_subj = remove_stop_words_and_punct(subj[0])\n",
        "            no_sw_obj = remove_stop_words_and_punct(obj[0])\n",
        "            # Add entries to the graph iff neither subject nor object is blank\n",
        "            if no_sw_subj and no_sw_obj:\n",
        "                tup = (no_sw_subj, verb_ls[index_min][0], no_sw_obj)\n",
        "                graph_tup_ls.append(tup)\n",
        "        \n",
        "        #clean_tup_ls = remove_dates(graph_tup_ls)\n",
        "    dedup_tup_ls = remove_duplicates(graph_tup_ls, 2)\n",
        "    clean_tup_ls = remove_dates(dedup_tup_ls)\n",
        "    return clean_tup_ls\n",
        "\n",
        "def get_obj_properties(tup_ls):\n",
        "    init_obj_tup_ls = []\n",
        "    for tup in tup_ls:\n",
        "        try:\n",
        "            text, node_label_ls, url = query_google(tup[2], api_key, limit=1)\n",
        "            new_tup = (tup[0], tup[1], tup[2], text[0], node_label_ls[0], url[0])\n",
        "        except:\n",
        "            new_tup = (tup[0], tup[1], tup[2], [], [], [])      \n",
        "        init_obj_tup_ls.append(new_tup)\n",
        "    return init_obj_tup_ls\n",
        "\n",
        "\n",
        "def add_layer(tup_ls):\n",
        "    svo_tup_ls = []\n",
        "    for tup in tup_ls:\n",
        "        if tup[3]:\n",
        "            svo_tup = create_svo_triples(tup[3])\n",
        "            svo_tup_ls.extend(svo_tup)\n",
        "        else:\n",
        "            continue\n",
        "    return get_obj_properties(svo_tup_ls)\n",
        "        \n",
        "\n",
        "def subj_equals_obj(tup_ls):\n",
        "    new_tup_ls = []\n",
        "    for tup in tup_ls:\n",
        "        if tup[0] != tup[2]:\n",
        "            new_tup_ls.append((tup[0], tup[1], tup[2], tup[3], tup[4], tup[5]))      \n",
        "    return new_tup_ls\n",
        "\n",
        "\n",
        "def check_for_string_labels(tup_ls):\n",
        "    # This is for an edge case where the object does not get fully populated\n",
        "    # resulting in the node labels being assigned to string instead of list.\n",
        "    # This may not be strictly necessary and the lines using it are commnted out\n",
        "    # below.  Run this function if you come across this case.\n",
        "    clean_tup_ls = []\n",
        "    for el in tup_ls:\n",
        "        if isinstance(el[2], list):\n",
        "            clean_tup_ls.append(el)\n",
        "    return clean_tup_ls\n",
        "\n",
        "\n",
        "def create_word_vectors(tup_ls):\n",
        "    new_tup_ls = []\n",
        "    for tup in tup_ls:\n",
        "        if tup[3]:\n",
        "            doc = nlp(tup[3])\n",
        "            new_tup = (tup[0], tup[1], tup[2], tup[3], tup[4], tup[5], doc.vector)\n",
        "        else:\n",
        "            new_tup = (tup[0], tup[1], tup[2], tup[3], tup[4], tup[5], np.random.uniform(low=-1.0, high=1.0, size=(300,)))\n",
        "        new_tup_ls.append(new_tup)\n",
        "    return new_tup_ls\n",
        "\n",
        "def dedup(tup_ls):\n",
        "    visited = set()\n",
        "    output_ls = []\n",
        "    \n",
        "    for tup in tup_ls:\n",
        "        if not tup[0] in visited:\n",
        "            visited.add(tup[0])\n",
        "            output_ls.append((tup[0], tup[1], tup[2], tup[3], tup[4]))        \n",
        "    return output_ls\n",
        "\n",
        "\n",
        "def convert_vec_to_ls(tup_ls):    \n",
        "    vec_to_ls_tup = []\n",
        "    for el in tup_ls:\n",
        "        vec_ls = [float(v) for v in el[4]]\n",
        "        tup = (el[0], el[1], el[2], el[3], vec_ls)\n",
        "        vec_to_ls_tup.append(tup)       \n",
        "    return vec_to_ls_tup\n",
        "\n",
        "\n",
        "def add_nodes(tup_ls):   \n",
        "    keys = ['name', 'description', 'node_labels', 'url', 'word_vec']\n",
        "    merge_nodes(graph.auto(), tup_ls, ('Node', 'name'), keys=keys)\n",
        "    print('Number of nodes in graph: ', graph.nodes.match('Node').count())\n",
        "    return\n",
        "    \n",
        "def add_edges(edge_ls):\n",
        "    edge_dc = {} \n",
        "    # Group tuple by verb\n",
        "    # Result: {verb1: [(sub1, v1, obj1), (sub2, v2, obj2), ...],\n",
        "    #          verb2: [(sub3, v3, obj3), (sub4, v4, obj4), ...]}\n",
        "    \n",
        "    for tup in edge_ls: \n",
        "        if tup[1] in edge_dc: \n",
        "            edge_dc[tup[1]].append((tup[0], tup[1], tup[2])) \n",
        "        else: \n",
        "            edge_dc[tup[1]] = [(tup[0], tup[1], tup[2])] \n",
        "    \n",
        "    for edge_labels, tup_ls in tqdm(edge_dc.items()):   # k=edge labels, v = list of tuples  \n",
        "        tx = graph.begin()\n",
        "        for el in tup_ls:\n",
        "            source_node = nodes_matcher.match(name=el[0]).first()\n",
        "            target_node = nodes_matcher.match(name=el[2]).first()\n",
        "            if not source_node:\n",
        "                source_node = Node('Node', name=el[0])\n",
        "                tx.create(source_node)\n",
        "            if not target_node:\n",
        "                try:\n",
        "                    target_node = Node('Node', name=el[2], node_labels=el[4], url=el[5], word_vec=el[6])\n",
        "                    tx.create(target_node)\n",
        "                except:\n",
        "                    continue\n",
        "            try:\n",
        "                rel = Relationship(source_node, edge_labels, target_node)\n",
        "            except:\n",
        "                continue\n",
        "            tx.create(rel)\n",
        "        tx.commit()\n",
        "    \n",
        "    return"
      ],
      "outputs": [],
      "metadata": {
        "id": "G9zRLI8rqn6e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Now we are going to create the nodes and edges for the extracted data. To do so, I have combined several of the steps in the previous cell into a 2 heloper functions below.**"
      ],
      "metadata": {
        "id": "3pselFqwqn6h"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "source": [
        "def edge_tuple_creation(text):\n",
        "    \n",
        "    initial_tup_ls = create_svo_triples(text)\n",
        "    init_obj_tup_ls = get_obj_properties(initial_tup_ls)\n",
        "    new_layer_ls = add_layer(init_obj_tup_ls)\n",
        "    starter_edge_ls = init_obj_tup_ls + new_layer_ls\n",
        "    edge_ls = subj_equals_obj(starter_edge_ls)\n",
        "    edges_word_vec_ls = create_word_vectors(edge_ls)\n",
        "    \n",
        "    return edges_word_vec_ls\n",
        "\n",
        "\n",
        "def node_tuple_creation(edges_word_vec_ls):\n",
        "    \n",
        "    orig_node_tup_ls = [(edges_word_vec_ls[0][0], '', ['Subject'], '', np.random.uniform(low=-1.0, high=1.0, size=(300,)))]\n",
        "    obj_node_tup_ls = [(tup[2], tup[3], tup[4], tup[5], tup[6]) for tup in edges_word_vec_ls]\n",
        "    full_node_tup_ls = orig_node_tup_ls + obj_node_tup_ls\n",
        "    cleaned_node_tup_ls = check_for_string_labels(full_node_tup_ls)\n",
        "    #dedup_node_tup_ls = dedup(cleaned_node_tup_ls)\n",
        "    dedup_node_tup_ls = cleaned_node_tup_ls\n",
        "    node_tup_ls = convert_vec_to_ls(dedup_node_tup_ls)\n",
        "    \n",
        "    return node_tup_ls    "
      ],
      "outputs": [],
      "metadata": {
        "id": "Q8XgypLTqn6h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **CONNECT TO NEO4J SANDBOX and Create Grpah**"
      ],
      "metadata": {
        "id": "UcELX95A3_22"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "source": [
        "# If you are using a Sandbox instance, you will want to use the following (commented) line.  \n",
        "# If you are using a Docker container for your DB, use the uncommented line.\n",
        "\n",
        "graph = Graph(\"bolt://44.203.186.12:7687\", name=\"neo4j\", password=\"stone-electrons-leakages\")\n",
        "\n",
        "nodes_matcher = NodeMatcher(graph)"
      ],
      "outputs": [],
      "metadata": {
        "id": "YeRxYsoPqn6h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#delete all exisitng nodes & relationships of the graph\n",
        "graph.run(\"MATCH (n) DETACH DELETE n\")"
      ],
      "metadata": {
        "id": "cPcjipjqWt5H",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d7ef7954-2861-4797-ccdb-691a862e2849"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(No data)"
            ],
            "text/html": [
              "(No data)"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PASS THE SCRAPED DATA TO BUILD GRAPH**"
      ],
      "metadata": {
        "id": "SoCCCeOG4EED"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "amalgamated_data =correct_company_info"
      ],
      "metadata": {
        "id": "FLj77DHetriu"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "source": [
        "%%time\n",
        "data = amalgamated_data\n",
        "data_edges_word_vec_ls = edge_tuple_creation(data)\n",
        "data_node_tup_ls = node_tuple_creation(data_edges_word_vec_ls)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 48.2 s, sys: 391 ms, total: 48.6 s\n",
            "Wall time: 1min 8s\n"
          ]
        }
      ],
      "metadata": {
        "id": "ijHnJ9vuqn6h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9bfe1cf-80f1-4d25-95ad-db71c0e5984c"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "source": [
        "full_node_ls = data_node_tup_ls\n",
        "full_edge_ls = data_edges_word_vec_ls\n",
        "full_dedup_node_tup_ls = dedup(full_node_ls)\n",
        "print(len(full_node_ls), len(full_dedup_node_tup_ls))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "204 190\n"
          ]
        }
      ],
      "metadata": {
        "id": "WM1nKLfXqn6h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "489a316d-6109-4837-d896-61f71eb6f895"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "source": [
        "add_nodes(full_dedup_node_tup_ls)\n",
        "add_edges(full_edge_ls)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of nodes in graph:  190\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/43 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:234: DeprecationWarning: The transaction.commit() method is deprecated, use graph.commit(transaction) instead\n",
            "100%|██████████| 43/43 [00:29<00:00,  1.46it/s]\n"
          ]
        }
      ],
      "metadata": {
        "id": "Q_jJC9Hwqn6i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed38ac9d-a9c6-47df-8b74-e7a842f3103e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **RUN CYPHER QUERY TO LIST TO ALL NODES**"
      ],
      "metadata": {
        "id": "4aD_7bzJ4Yp8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = graph.run(\"MATCH (n) RETURN n\").data()"
      ],
      "metadata": {
        "id": "fZTksQX84cP8"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data[:10]"
      ],
      "metadata": {
        "id": "COle7fRF51Yw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92cf9268-a1f8-49da-c297-11a8f23b0f5c"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'n': Node('Node', name='study skills')},\n",
              " {'n': Node('Node', name='iphone')},\n",
              " {'n': Node('Node', name='iphone')},\n",
              " {'n': Node('Node', name='software')},\n",
              " {'n': Node('Node', name='software')},\n",
              " {'n': Node('Node', name='software')},\n",
              " {'n': Node('Node', name='software')},\n",
              " {'n': Node('Node', name='sql update statement')},\n",
              " {'n': Node('Node', name='sql update statement')},\n",
              " {'n': Node('Node', description='', name='artificial intelligence', node_labels=['Subject'], url='', word_vec=[0.49684890555669803, -0.9017952207275155, -0.5658928729874544, 0.9665148488811013, -0.12313715369979383, -0.5105621231681874, -0.3636860515499085, 0.34671870175580777, -0.6884420420057338, -0.5933094088321593, 0.5090555845310403, 0.9866194652978661, 0.6220325734886627, 0.21796024369148514, -0.0733360884635832, -0.15005156659472796, 0.2630632354611868, 0.5690905865714369, -0.6538415249276432, 0.42421556578918396, -0.37165647898171916, 0.2626629880975788, -0.7354614022778883, 0.11162816742239112, -0.6331538090067086, -0.1311112838212216, -0.6772193491019682, -0.2574881473391062, 0.6923651205135783, 0.6381812521076984, -0.1227715249598742, 0.538809085850853, 0.2627733301666886, 0.8202670875836091, -0.8485912857221722, -0.14592850082606534, 0.4205095225381452, -0.019998509017484123, 0.08170184449720419, -0.17986632799616964, 0.12904857983929863, 0.9635675180537664, 0.38643727384556903, 0.28902255425388734, 0.29932608207044975, 0.2709632984581518, 0.5931882624430691, -0.37222212797555954, -0.9349661813624068, -0.8926912963659033, -0.34880951821368433, -0.2871427508354343, -0.21909391172768733, 0.5494144183269372, 0.9627147760090438, 0.20583003694528879, -0.2920984185007487, 0.6513293232379889, 0.9930397898663503, -0.5935469167817322, 0.18195367516404026, -0.49697873863099296, -0.02046208566620744, 0.013527380794589705, -0.40026106980069587, 0.7070832409687222, 0.047320332052963154, 0.4911391340469873, -0.3461107247113784, 0.6788931055084448, 0.15330526697679425, -0.8164016204660267, -0.9036134735924035, 0.7804221794746122, 0.21366704303474404, -0.18350563429517575, 0.8703816048834601, -0.7116425517660365, 0.44339226101700135, 0.16706300820084552, 0.0160083983852064, -0.9227601984107636, 0.33653817185941715, -0.5442948016861096, 0.8610591807254966, -0.9439149763031129, -0.4577034026090683, -0.6743929211426378, 0.4035928215359077, 0.575339197257773, 0.1307031765078046, 0.495899716665293, -0.20897586293111914, 0.3085890029634346, 0.42335450588615964, 0.6755504422799155, 0.051440561736966206, -0.3182565021942847, -0.3082179598712991, 0.6300921195568354, -0.058211517910256916, -0.046930001683727074, 0.9629850382736143, -0.0005925234228643106, 0.271796080230988, 0.14356743019950757, -0.28102620938305245, -0.31456212401130523, 0.3870788291801237, -0.5463179831698619, -0.2938430770219236, -0.5818766126406545, -0.826545527653348, 0.1655438224236676, 0.10515513177433866, -0.16035540525234016, -0.3721743016808621, -0.11304309150863867, 0.9909577883463385, -0.04361722917112876, -0.7933588718492988, -0.7854096218205218, 0.38227881249121487, -0.561311502379747, 0.01730555574518955, 0.4057926556233049, 0.5899428463231049, -0.9420356908123788, -0.5299418771930435, 0.1818923934118184, 0.9438212638298515, 0.9458665427197239, -0.09063742434159616, -0.18584283370753352, -0.9150867286853643, 0.502136020021368, 0.672920522221256, 0.1579421050312413, -0.43161862302420073, -0.5272128161677849, 0.23244255570095973, 0.31907457948933704, -0.9722074921634414, -0.7629202231336627, -0.02119808593004735, 0.21670825627443402, 0.8249071247982995, -0.2549263577938814, -0.46204080327915453, -0.0315115531508956, 0.5513657573522015, -0.13545262343207787, -0.5294424952599215, -0.6745617394251577, -0.14248668378255536, 0.16498086046544147, 0.44116487692530737, -0.17403294149839676, -0.7823928497563954, 0.47627368308445983, 0.40033334587569436, 0.8082266120744082, -0.9497661206786441, -0.21726361774826564, -0.016700767156697882, -0.05924561132399431, 0.6166729691188504, -0.33087863285386954, -0.2213318826990749, 0.335240441243964, -0.31781690172874244, -0.5884392671166785, 0.8238501088392245, 0.1685227555027451, -0.14819669756708165, 0.2032632932931404, 0.4489161315351504, -0.3905695611167834, 0.7386617685820833, 0.44703083452482906, -0.5754228440968645, -0.9493586838400221, 0.42392223368609483, -0.7337118479001332, 0.8373259596524365, 0.7397287895384941, 0.8964953137817264, 0.10803468622031409, -0.1861516263089924, 0.12586943788252913, -0.599825138504847, 0.8026446006527848, 0.6701384468364973, 0.7973287873048001, 0.7012072634276159, -0.1346107775477008, -0.29188122990406073, 0.7101398221792707, 0.7468307562395609, 0.0005334958195071238, 0.48087086135945234, 0.05301295409047335, -0.3964158447431294, -0.582315881271996, -0.692800136380963, 0.30398466882806896, -0.35775368029207866, 0.7873694059971603, -0.6742197312245986, -0.07242178044946312, 0.5075837265742615, -0.4255880788867008, -0.10412243834644008, 0.8187505957326651, 0.39489862828294653, -0.9391250234316355, 0.2718187304946251, -0.7586945924276656, 0.04937957663315817, 0.4281047404641649, -0.07724917134627174, 0.31642153531033945, -0.5991902047457682, 0.42200810374098996, -0.18674087356971492, 0.9674076278843153, -0.11233575007077867, 0.5394514619360951, 0.04546400791959049, 0.4433387519430725, -0.3245887224521513, 0.6785287489738301, 0.55919913329357, -0.9254489398008179, -0.7495820004286953, 0.5214539393038131, 0.2254235838130263, 0.8715453577775496, 0.36294650351704116, -0.5095662231846272, 0.8705185530118356, 0.9687404072772838, 0.15946512633080445, -0.7676862295651983, 0.4429610875884109, -0.026400965672676913, -0.0019873307666451456, -0.015415226842017793, 0.6209782123688068, 0.9979914759076487, 0.735601745456808, -0.7938779551252308, 0.6784786714783257, -0.21992843871682122, -0.2615226932367054, -0.8687463047905815, -0.31757305617444165, 0.9980487699380558, -0.6161712847801863, 0.22992452832586951, 0.48709420011301785, 0.011156988956105174, -0.6838438401823843, 0.950214693784553, 0.7290455400976659, -0.7235725969281319, -0.4724153986046291, -0.6717335738143058, 0.9701011161951116, 0.5521009216657362, -0.4798085458330499, 0.6210396929154591, 0.07657594392314904, -0.8808168025943162, 0.6197748906819613, -0.050585850234237784, -0.31456592243405535, 0.7412780263015584, -0.3777454428002365, -0.9591921574157041, -0.013373816100491931, 0.5207884710620752, 0.48719811614339315, -0.2728975332275372, 0.10924902961319716, -0.6519735727202316, 0.3130242966229697, -0.5847175143606005, -0.607839864633674, -0.7052021728554678, 0.4994563368091385, 0.9712565581036483, 0.6985978630845093, -0.6830226370480967, -0.32284128690015, -0.9509904950910986, 0.8307091817944083, 0.8909725523578189, -0.6040582137259334, -0.6296628946458858])}]"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RUN CYPTHER QUERY TO LIST ALL NODES AND ADD NODE LABELS**"
      ],
      "metadata": {
        "id": "iRKpPUia0Vg5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_labels = graph.run(\"MATCH (n:Node) CALL apoc.create.addLabels(n, n.node_labels) YIELD node RETURN node\").data()"
      ],
      "metadata": {
        "id": "_xaAzkFh0Z4K"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_labels[:10]"
      ],
      "metadata": {
        "id": "knqqT0-KCk8h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1338e541-66fc-4ea5-bf8f-044204ceb36a"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'node': Node('Node', name='study skills')},\n",
              " {'node': Node('Node', name='iphone')},\n",
              " {'node': Node('Node', name='iphone')},\n",
              " {'node': Node('Node', name='software')},\n",
              " {'node': Node('Node', name='software')},\n",
              " {'node': Node('Node', name='software')},\n",
              " {'node': Node('Node', name='software')},\n",
              " {'node': Node('Node', name='sql update statement')},\n",
              " {'node': Node('Node', name='sql update statement')},\n",
              " {'node': Node('Node', 'Subject', description='', name='artificial intelligence', node_labels=['Subject'], url='', word_vec=[0.49684890555669803, -0.9017952207275155, -0.5658928729874544, 0.9665148488811013, -0.12313715369979383, -0.5105621231681874, -0.3636860515499085, 0.34671870175580777, -0.6884420420057338, -0.5933094088321593, 0.5090555845310403, 0.9866194652978661, 0.6220325734886627, 0.21796024369148514, -0.0733360884635832, -0.15005156659472796, 0.2630632354611868, 0.5690905865714369, -0.6538415249276432, 0.42421556578918396, -0.37165647898171916, 0.2626629880975788, -0.7354614022778883, 0.11162816742239112, -0.6331538090067086, -0.1311112838212216, -0.6772193491019682, -0.2574881473391062, 0.6923651205135783, 0.6381812521076984, -0.1227715249598742, 0.538809085850853, 0.2627733301666886, 0.8202670875836091, -0.8485912857221722, -0.14592850082606534, 0.4205095225381452, -0.019998509017484123, 0.08170184449720419, -0.17986632799616964, 0.12904857983929863, 0.9635675180537664, 0.38643727384556903, 0.28902255425388734, 0.29932608207044975, 0.2709632984581518, 0.5931882624430691, -0.37222212797555954, -0.9349661813624068, -0.8926912963659033, -0.34880951821368433, -0.2871427508354343, -0.21909391172768733, 0.5494144183269372, 0.9627147760090438, 0.20583003694528879, -0.2920984185007487, 0.6513293232379889, 0.9930397898663503, -0.5935469167817322, 0.18195367516404026, -0.49697873863099296, -0.02046208566620744, 0.013527380794589705, -0.40026106980069587, 0.7070832409687222, 0.047320332052963154, 0.4911391340469873, -0.3461107247113784, 0.6788931055084448, 0.15330526697679425, -0.8164016204660267, -0.9036134735924035, 0.7804221794746122, 0.21366704303474404, -0.18350563429517575, 0.8703816048834601, -0.7116425517660365, 0.44339226101700135, 0.16706300820084552, 0.0160083983852064, -0.9227601984107636, 0.33653817185941715, -0.5442948016861096, 0.8610591807254966, -0.9439149763031129, -0.4577034026090683, -0.6743929211426378, 0.4035928215359077, 0.575339197257773, 0.1307031765078046, 0.495899716665293, -0.20897586293111914, 0.3085890029634346, 0.42335450588615964, 0.6755504422799155, 0.051440561736966206, -0.3182565021942847, -0.3082179598712991, 0.6300921195568354, -0.058211517910256916, -0.046930001683727074, 0.9629850382736143, -0.0005925234228643106, 0.271796080230988, 0.14356743019950757, -0.28102620938305245, -0.31456212401130523, 0.3870788291801237, -0.5463179831698619, -0.2938430770219236, -0.5818766126406545, -0.826545527653348, 0.1655438224236676, 0.10515513177433866, -0.16035540525234016, -0.3721743016808621, -0.11304309150863867, 0.9909577883463385, -0.04361722917112876, -0.7933588718492988, -0.7854096218205218, 0.38227881249121487, -0.561311502379747, 0.01730555574518955, 0.4057926556233049, 0.5899428463231049, -0.9420356908123788, -0.5299418771930435, 0.1818923934118184, 0.9438212638298515, 0.9458665427197239, -0.09063742434159616, -0.18584283370753352, -0.9150867286853643, 0.502136020021368, 0.672920522221256, 0.1579421050312413, -0.43161862302420073, -0.5272128161677849, 0.23244255570095973, 0.31907457948933704, -0.9722074921634414, -0.7629202231336627, -0.02119808593004735, 0.21670825627443402, 0.8249071247982995, -0.2549263577938814, -0.46204080327915453, -0.0315115531508956, 0.5513657573522015, -0.13545262343207787, -0.5294424952599215, -0.6745617394251577, -0.14248668378255536, 0.16498086046544147, 0.44116487692530737, -0.17403294149839676, -0.7823928497563954, 0.47627368308445983, 0.40033334587569436, 0.8082266120744082, -0.9497661206786441, -0.21726361774826564, -0.016700767156697882, -0.05924561132399431, 0.6166729691188504, -0.33087863285386954, -0.2213318826990749, 0.335240441243964, -0.31781690172874244, -0.5884392671166785, 0.8238501088392245, 0.1685227555027451, -0.14819669756708165, 0.2032632932931404, 0.4489161315351504, -0.3905695611167834, 0.7386617685820833, 0.44703083452482906, -0.5754228440968645, -0.9493586838400221, 0.42392223368609483, -0.7337118479001332, 0.8373259596524365, 0.7397287895384941, 0.8964953137817264, 0.10803468622031409, -0.1861516263089924, 0.12586943788252913, -0.599825138504847, 0.8026446006527848, 0.6701384468364973, 0.7973287873048001, 0.7012072634276159, -0.1346107775477008, -0.29188122990406073, 0.7101398221792707, 0.7468307562395609, 0.0005334958195071238, 0.48087086135945234, 0.05301295409047335, -0.3964158447431294, -0.582315881271996, -0.692800136380963, 0.30398466882806896, -0.35775368029207866, 0.7873694059971603, -0.6742197312245986, -0.07242178044946312, 0.5075837265742615, -0.4255880788867008, -0.10412243834644008, 0.8187505957326651, 0.39489862828294653, -0.9391250234316355, 0.2718187304946251, -0.7586945924276656, 0.04937957663315817, 0.4281047404641649, -0.07724917134627174, 0.31642153531033945, -0.5991902047457682, 0.42200810374098996, -0.18674087356971492, 0.9674076278843153, -0.11233575007077867, 0.5394514619360951, 0.04546400791959049, 0.4433387519430725, -0.3245887224521513, 0.6785287489738301, 0.55919913329357, -0.9254489398008179, -0.7495820004286953, 0.5214539393038131, 0.2254235838130263, 0.8715453577775496, 0.36294650351704116, -0.5095662231846272, 0.8705185530118356, 0.9687404072772838, 0.15946512633080445, -0.7676862295651983, 0.4429610875884109, -0.026400965672676913, -0.0019873307666451456, -0.015415226842017793, 0.6209782123688068, 0.9979914759076487, 0.735601745456808, -0.7938779551252308, 0.6784786714783257, -0.21992843871682122, -0.2615226932367054, -0.8687463047905815, -0.31757305617444165, 0.9980487699380558, -0.6161712847801863, 0.22992452832586951, 0.48709420011301785, 0.011156988956105174, -0.6838438401823843, 0.950214693784553, 0.7290455400976659, -0.7235725969281319, -0.4724153986046291, -0.6717335738143058, 0.9701011161951116, 0.5521009216657362, -0.4798085458330499, 0.6210396929154591, 0.07657594392314904, -0.8808168025943162, 0.6197748906819613, -0.050585850234237784, -0.31456592243405535, 0.7412780263015584, -0.3777454428002365, -0.9591921574157041, -0.013373816100491931, 0.5207884710620752, 0.48719811614339315, -0.2728975332275372, 0.10924902961319716, -0.6519735727202316, 0.3130242966229697, -0.5847175143606005, -0.607839864633674, -0.7052021728554678, 0.4994563368091385, 0.9712565581036483, 0.6985978630845093, -0.6830226370480967, -0.32284128690015, -0.9509904950910986, 0.8307091817944083, 0.8909725523578189, -0.6040582137259334, -0.6296628946458858])}]"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Drop Duplicates if exist\n",
        "graph.run(\"MATCH (n:Node) WITH n.name AS name, COLLECT(n) AS nodes WHERE SIZE(nodes)>1 FOREACH (el in nodes | DETACH DELETE el)\").data()"
      ],
      "metadata": {
        "id": "NK6DaRk7FS80",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "306bb75b-6762-49c8-b9c1-bd2e79b26e5d"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **VISUALIZATION AND EXPORT TO DF NAD WRITE TO FILE**"
      ],
      "metadata": {
        "id": "5_24ZRQG6hji"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "LINK FOR VISUALIZATION: \n",
        "<a href=\"https://12b8dc4f360aa15ea38c9041bbd1d97d.neo4jsandbox.com/browser/?token=pwfetch:12b8dc4f360aa15ea38c9041bbd1d97d:eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCIsImtpZCI6IlFUbENPRVV4UmtJNFJETkROakpETXpBME5EZzBRelV3UWpNek9UVTVNRFF4TlRKRk56STJOZyJ9.eyJlbWFpbCI6ImF2aW5hc2gucmFtZXNoQHNqc3UuZWR1IiwiZmFtaWx5X25hbWUiOiJSYW1lc2giLCJnaXZlbl9uYW1lIjoiQXZpbmFzaCIsImxvY2FsZSI6ImVuIiwibmFtZSI6IkF2aW5hc2ggUmFtZXNoIiwibmlja25hbWUiOiJhdmluYXNoLnJhbWVzaCIsInBpY3R1cmUiOiJodHRwczovL2xoMy5nb29nbGV1c2VyY29udGVudC5jb20vYS9BQVRYQUp3SXNjMDFPQWtzcFRiTnVZM1A5bHBHMkl2RzktY3VGMHVhVzV3PXM5Ni1jIiwidXNlcl9tZXRhZGF0YSI6eyJjb21wYW55IjoiU3R1ZGVudCJ9LCJhcHBfbWV0YWRhdGEiOnsic2FuZGJveHYzIjp7ImNyZWF0ZWRBdCI6MTY1MzMzNzQ1ODIyMiwiYWdyZWVkVG9UZXJtc0F0IjoxNjUzMzM3NDY2NzQ0fX0sInNhbmRib3h2MyI6eyJjcmVhdGVkQXQiOjE2NTMzMzc0NTgyMjIsImFncmVlZFRvVGVybXNBdCI6MTY1MzMzNzQ2Njc0NH0sImZpcmViYXNlX2RhdGEiOnsidWlkIjoiZ29vZ2xlLW9hdXRoMnwxMTgxODc1NDA3MDUxMzcwMzY0MzcifSwic2NvcGVzIjp7InNhbmRib3hlcyI6WyJzYm94MSIsInNib3gyIiwic2JveDMiXX0sImNsaWVudElEIjoiRHhobWlGOFRDZXpuSTdYb2kwOFV5WVNjTEdabms0a2UiLCJjcmVhdGVkX2F0IjoiMjAyMi0wNS0yM1QyMDoyNDoxNi4zNDVaIiwiZW1haWxfdmVyaWZpZWQiOnRydWUsImlkZW50aXRpZXMiOlt7InByb3ZpZGVyIjoiZ29vZ2xlLW9hdXRoMiIsInVzZXJfaWQiOiIxMTgxODc1NDA3MDUxMzcwMzY0MzciLCJjb25uZWN0aW9uIjoiZ29vZ2xlLW9hdXRoMiIsImlzU29jaWFsIjp0cnVlfV0sInVwZGF0ZWRfYXQiOiIyMDIyLTA1LTI5VDE1OjM2OjQ3LjA1OVoiLCJ1c2VyX2lkIjoiZ29vZ2xlLW9hdXRoMnwxMTgxODc1NDA3MDUxMzcwMzY0MzciLCJpc3MiOiJodHRwczovL2xvZ2luLm5lbzRqLmNvbS8iLCJzdWIiOiJnb29nbGUtb2F1dGgyfDExODE4NzU0MDcwNTEzNzAzNjQzNyIsImF1ZCI6IkR4aG1pRjhUQ2V6bkk3WG9pMDhVeVlTY0xHWm5rNGtlIiwiaWF0IjoxNjUzOTIyNjQyLCJleHAiOjE2NTQwMDkwNDIsIm5vbmNlIjoiWVhnME4yRnBhRFZDYkc1VE1GSmpZM2xtV0c4d2RsZGlTRmxUUkVaeVlrcDZYMzR4Tlc5c2VUTTRVUT09In0.NrMKp-dD71dYVbv10N2NO6u3Sveh8KVDBMxUf8-uHfp37J-u4GXSvXFDz7aydkTgd-HJwxy8mnewqiIBp5xl8IcVB3gIYzEi1SmNyUXHtBx7BR7rFeO82s4k-yEn-Oz1q1p2f3SMmldJag4FEGwdCLdErya_UrOv6BF1hgf_gPI9nhmqA7iy6vDKmZiIicDAQtKt_KImCc326o8LS4Qb6HLibJlRBTQXB7cEeTW77WWWgoFaL2Iy5MLat12wJum5FY74F-V1Yqy09CeaOCheJlKrxuJkheWCFe0UBggkQoLgumhVKz6aTiOE2q5z4IqrOVP87XTna4Grh1XfsnojmA\"> Click here to visualize the graph in Neo4j Browser</a>\n",
        "\n",
        "\n",
        "RUN THE BELOW CYPHER QUERY to GET GRAPH OUTPUTS\n",
        "\n",
        "```MATCH (n) RETURN n```\n"
      ],
      "metadata": {
        "id": "SsD55jlREBEv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SAMPLE KNOWLEDGE GRAPH VISUALIZATION SNIPPET FOR A COMPANY - Quillbot, Coinbase**"
      ],
      "metadata": {
        "id": "3_3g0E25EWtt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<center><b>Quillbot KG</b></center>\n",
        "\n",
        "                            \n",
        "![\"Quillbot Example\"](https://github.com/rameshavinash94/cmpe257_final_exam/blob/main/graph%20(1).png?raw=true)\n",
        "\n",
        "<center><b>COINBASE KG</b></center>\n",
        "\n",
        "\n",
        "![\"CoinBase Example\"](https://github.com/rameshavinash94/cmpe257_final_exam/blob/main/graph%20(2).png?raw=true)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "C-m6S8O1Qj-l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TRAVERSE GRAPH FROM NEO4J AND PRINT NODE-RELATIONSHIP IN DATAFRAME**"
      ],
      "metadata": {
        "id": "2jPrAyXi9oid"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install neo4j"
      ],
      "metadata": {
        "id": "V5agMb5FSOlH",
        "outputId": "ac4e6c58-4792-4836-8a00-9f7531ab1e5c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: neo4j in /usr/local/lib/python3.7/dist-packages (4.4.3)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from neo4j) (2022.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from neo4j import GraphDatabase, basic_auth\n",
        "\n",
        "driver = GraphDatabase.driver(\n",
        "  \"bolt://44.203.186.12:7687\",\n",
        "  auth=basic_auth(\"neo4j\", \"stone-electrons-leakages\"))"
      ],
      "metadata": {
        "id": "RhvXdKLrAldi"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cypher_query = '''\n",
        "Match (n)-[r]->(m)\n",
        "Return n.name,r,m.name\n",
        "'''\n",
        "with driver.session(database=\"neo4j\") as session:\n",
        "  results = session.read_transaction(\n",
        "    lambda tx: tx.run(cypher_query,\n",
        "                      limit=\"10\").data())\n",
        "driver.close()"
      ],
      "metadata": {
        "id": "HLd6nOjfNNMI"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "taB-Fp118yTC",
        "outputId": "a05d9fac-46bf-48b2-c615-9e36a48b8f6b"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'m.name': 'new variations',\n",
              " 'n.name': 'artificial intelligence',\n",
              " 'r': ({}, 'be', {})}"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "source=[]\n",
        "destination=[]\n",
        "relationship=[]\n",
        "for x in range(len(results)):\n",
        "  relationship.append(results[x]['r'][1])\n",
        "  source.append(results[x]['n.name'])\n",
        "  destination.append(results[x]['m.name'])"
      ],
      "metadata": {
        "id": "u7RBGH-DPu99"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "graph_data = pd.DataFrame({\"source\":source,\"relationship\":relationship,\"destination\":destination})"
      ],
      "metadata": {
        "id": "hprxbbmT2AJD"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "graph_data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "WXYP_6lF-zkM",
        "outputId": "141346a5-c087-49ef-b1ad-f82be956f1ad"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                    source relationship                    destination\n",
              "0  artificial intelligence           be                 new variations\n",
              "1  artificial intelligence           be                         update\n",
              "2  artificial intelligence           be              paraphrasing tool\n",
              "3  artificial intelligence           be                   intelligence\n",
              "4  artificial intelligence           be  previous sure capital article"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-822b5e53-3eeb-4adf-8771-bb3b2707a0da\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>source</th>\n",
              "      <th>relationship</th>\n",
              "      <th>destination</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>artificial intelligence</td>\n",
              "      <td>be</td>\n",
              "      <td>new variations</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>artificial intelligence</td>\n",
              "      <td>be</td>\n",
              "      <td>update</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>artificial intelligence</td>\n",
              "      <td>be</td>\n",
              "      <td>paraphrasing tool</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>artificial intelligence</td>\n",
              "      <td>be</td>\n",
              "      <td>intelligence</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>artificial intelligence</td>\n",
              "      <td>be</td>\n",
              "      <td>previous sure capital article</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-822b5e53-3eeb-4adf-8771-bb3b2707a0da')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-822b5e53-3eeb-4adf-8771-bb3b2707a0da button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-822b5e53-3eeb-4adf-8771-bb3b2707a0da');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**WRITE TO TEXT FILE**"
      ],
      "metadata": {
        "id": "U0L0UgZA-T7W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "graph_data.to_csv('final_graph_traversed_data.txt',sep=' ',header=\"False\",index=False)"
      ],
      "metadata": {
        "id": "-TVbCy9H9fcv"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **SECTION 3 - COMPANY INFO SUMMARIZATION**"
      ],
      "metadata": {
        "id": "8j4Ai7Fc6Gqf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "sU2lcflTxnbO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5c4afc6-1d38-4145-aa9a-92c9db8385b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "Quillbot:support@crunchbase.com.support@crunchbase.com.6 Examples Of Marketing Tools Using Artificial Intelligence (AI)\n",
            "Artificial intelligence (AI) is quickly becoming one of the most popular marketing buzzwords\n",
            "And for good reason – AI can help us automate repetitive tasks, personalize marketing efforts, and make better decisions, faster! When it comes to increasing brand v…Quillbot Review\n",
            "Do you want to write a better version of your online content? Quillbot is a paraphrasing tool that switches your text to make it unique and better—almost like a thesaurus for complete sentences! As a bestselling author and content creator, I love programs like…Best Online Spellchecker – Free and Paid Options\n",
            "Have you experienced publishing a blog post only to find out there’s a typo in the introduction? Then you should find a program that corrects these writing errors\n",
            "Whether you write for business or academic purposes, checking your spelling and grammar is ALW…Best Free and Paid Plagiarism Checkers\n",
            "Running your work on a plagiarism checker to look for duplicate content is essential before publishing it online\n",
            "Otherwise, you can get sued for intellectual fraud or theft\n",
            "I carefully selected the best plagiarism checker for students, professional writers…New Developments At Sure Capital\n",
            "This is an update to a previous Sure Capital article I published in December 2020\n",
            "Read more to see the fund performance, recent distribution, and portfolio holdings.Sure Capital's (SSSS) CEO Mark Klein on Q1 2022 Results - Earnings Call Transcript\n",
            "SuRo Capital Corp\n",
            "(NASDAQ:NASDAQ:SASS) Q1 2022 Earnings Conference Call May 4, 2022 5:00 PM Company Participants Willy Lee – Investor Relations Mark Klein – Chairman and Chief Executive...7 AI-Powered Content Creation Tools for Social Media Managers.AI-powered content creation tools can't replace great writers — but they help writers and marketers save time and use their skills for more strategic aspects of content creation.Use a Paraphrase Generator and Reword Your Marketing Copy\n",
            "Coming up with New Variations for your Marketing Copy is hard! Let AI do the heavy lifting\n",
            "Use a Paraphrase Generator and Reword Your Marketing Copy.Name is Quillbot\n",
            "Size is 11-50.id is Quillbot\n",
            "Industry is computer software.LinkedIn_URL is linkedin.com/company/quillbot.linkedin_id is 14811938.website is quillbot.com.type is private\n",
            "Raw is ['Quillbot'].score is 1.0.fuzzy_match is True.\n",
            "[(0.10421581858389978, ['(NASDAQ:NASDAQ:SASS)', 'Q1', '2022', 'Earnings', 'Conference', 'Call', 'May', '4,', '2022', '5:00', 'PM', 'Company', 'Participants', 'Willy', 'Lee', '–', 'Investor', 'Relations', 'Mark', 'Klein', '–', 'Chairman', 'and', 'Chief', 'Executive...7', 'AI-Powered', 'Content', 'Creation', 'Tools', 'for', 'Social', 'Media', 'Managers.AI-powered', 'content', 'creation', 'tools', \"can't\", 'replace', 'great', 'writers', '—', 'but', 'they', 'help', 'writers', 'and', 'marketers', 'save', 'time', 'and', 'use', 'their', 'skills', 'for', 'more', 'strategic', 'aspects', 'of', 'content', 'creation.Use', 'a', 'Paraphrase', 'Generator', 'and', 'Reword', 'Your', 'Marketing', 'Copy']), (0.09845327613169333, ['Use', 'a', 'Paraphrase', 'Generator', 'and', 'Reword', 'Your', 'Marketing', 'Copy.Name', 'is', 'Quillbot']), (0.08825383421153363, ['Quillbot:support@crunchbase.com.support@crunchbase.com.6', 'Examples', 'Of', 'Marketing', 'Tools', 'Using', 'Artificial', 'Intelligence', '(AI)']), (0.0871640767485787, ['I', 'carefully', 'selected', 'the', 'best', 'plagiarism', 'checker', 'for', 'students,', 'professional', 'writers…New', 'Developments', 'At', 'Sure', 'Capital']), (0.0841600150729155, ['Running', 'your', 'work', 'on', 'a', 'plagiarism', 'checker', 'to', 'look', 'for', 'duplicate', 'content', 'is', 'essential', 'before', 'publishing', 'it', 'online']), (0.08090458083094582, ['Do', 'you', 'want', 'to', 'write', 'a', 'better', 'version', 'of', 'your', 'online', 'content?', 'Quillbot', 'is', 'a', 'paraphrasing', 'tool', 'that', 'switches', 'your', 'text', 'to', 'make', 'it', 'unique', 'and', 'better—almost', 'like', 'a', 'thesaurus', 'for', 'complete', 'sentences!', 'As', 'a', 'bestselling', 'author', 'and', 'content', 'creator,', 'I', 'love', 'programs', 'like…Best', 'Online', 'Spellchecker', '–', 'Free', 'and', 'Paid', 'Options']), (0.07857010606983555, ['Artificial', 'intelligence', '(AI)', 'is', 'quickly', 'becoming', 'one', 'of', 'the', 'most', 'popular', 'marketing', 'buzzwords']), (0.06211873713278091, ['And', 'for', 'good', 'reason', '–', 'AI', 'can', 'help', 'us', 'automate', 'repetitive', 'tasks,', 'personalize', 'marketing', 'efforts,', 'and', 'make', 'better', 'decisions,', 'faster!', 'When', 'it', 'comes', 'to', 'increasing', 'brand', 'v…Quillbot', 'Review']), (0.05822848421262573, ['This', 'is', 'an', 'update', 'to', 'a', 'previous', 'Sure', 'Capital', 'article', 'I', 'published', 'in', 'December', '2020']), (0.057119247448479384, ['Coming', 'up', 'with', 'New', 'Variations', 'for', 'your', 'Marketing', 'Copy', 'is', 'hard!', 'Let', 'AI', 'do', 'the', 'heavy', 'lifting']), (0.05350829660340043, ['SuRo', 'Capital', 'Corp']), (0.043368354864753676, ['Whether', 'you', 'write', 'for', 'business', 'or', 'academic', 'purposes,', 'checking', 'your', 'spelling', 'and', 'grammar', 'is', 'ALW…Best', 'Free', 'and', 'Paid', 'Plagiarism', 'Checkers']), (0.038597339565679886, ['Size', 'is', '11-50.id', 'is', 'Quillbot']), (0.026767821713313524, ['Read', 'more', 'to', 'see', 'the', 'fund', 'performance,', 'recent', 'distribution,', 'and', 'portfolio', 'holdings.Sure', \"Capital's\", '(SSSS)', 'CEO', 'Mark', 'Klein', 'on', 'Q1', '2022', 'Results', '-', 'Earnings', 'Call', 'Transcript']), (0.018962167672308995, ['Have', 'you', 'experienced', 'publishing', 'a', 'blog', 'post', 'only', 'to', 'find', 'out', 'there’s', 'a', 'typo', 'in', 'the', 'introduction?', 'Then', 'you', 'should', 'find', 'a', 'program', 'that', 'corrects', 'these', 'writing', 'errors']), (0.009803921568627453, ['Otherwise,', 'you', 'can', 'get', 'sued', 'for', 'intellectual', 'fraud', 'or', 'theft']), (0.009803921568627453, ['Industry', 'is', 'computer', 'software.LinkedIn_URL', 'is', 'linkedin.com/company/quillbot.linkedin_id', 'is', '14811938.website', 'is', 'quillbot.com.type', 'is', 'private'])]\n",
            "Summarize Text: \n",
            " (NASDAQ:NASDAQ:SASS) Q1 2022 Earnings Conference Call May 4, 2022 5:00 PM Company Participants Willy Lee – Investor Relations Mark Klein – Chairman and Chief Executive...7 AI-Powered Content Creation Tools for Social Media Managers.AI-powered content creation tools can't replace great writers — but they help writers and marketers save time and use their skills for more strategic aspects of content creation.Use a Paraphrase Generator and Reword Your Marketing Copy. Use a Paraphrase Generator and Reword Your Marketing Copy.Name is Quillbot. Quillbot:support@crunchbase.com.support@crunchbase.com.6 Examples Of Marketing Tools Using Artificial Intelligence (AI). I carefully selected the best plagiarism checker for students, professional writers…New Developments At Sure Capital. Running your work on a plagiarism checker to look for duplicate content is essential before publishing it online\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.cluster.util import cosine_distance\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        " \n",
        "def read_article(file_name):\n",
        "    file = open(file_name, \"r\")\n",
        "    filedata = file.readlines()\n",
        "    article = filedata[0].split(\". \")\n",
        "    sentences = []\n",
        "\n",
        "    for sentence in article:\n",
        "        print(sentence)\n",
        "        sentences.append(sentence.replace(\"[^a-zA-Z]\", \" \").split(\" \"))\n",
        "    sentences.pop() \n",
        "    \n",
        "    return sentences\n",
        "\n",
        "def sentence_similarity(sent1, sent2, stopwords=None):\n",
        "    if stopwords is None:\n",
        "        stopwords = []\n",
        " \n",
        "    sent1 = [w.lower() for w in sent1]\n",
        "    sent2 = [w.lower() for w in sent2]\n",
        " \n",
        "    all_words = list(set(sent1 + sent2))\n",
        " \n",
        "    vector1 = [0] * len(all_words)\n",
        "    vector2 = [0] * len(all_words)\n",
        " \n",
        "    # build the vector for the first sentence\n",
        "    for w in sent1:\n",
        "        if w in stopwords:\n",
        "            continue\n",
        "        vector1[all_words.index(w)] += 1\n",
        " \n",
        "    # build the vector for the second sentence\n",
        "    for w in sent2:\n",
        "        if w in stopwords:\n",
        "            continue\n",
        "        vector2[all_words.index(w)] += 1\n",
        " \n",
        "    return 1 - cosine_distance(vector1, vector2)\n",
        " \n",
        "\n",
        "def build_similarity_matrix(sentences, stop_words):\n",
        "    # Create an empty similarity matrix\n",
        "    similarity_matrix = np.zeros((len(sentences), len(sentences)))\n",
        " \n",
        "    for idx1 in range(len(sentences)):\n",
        "        for idx2 in range(len(sentences)):\n",
        "            if idx1 == idx2: #ignore if both are same sentences\n",
        "                continue \n",
        "            similarity_matrix[idx1][idx2] = sentence_similarity(sentences[idx1], sentences[idx2], stop_words)\n",
        "\n",
        "    return similarity_matrix\n",
        "\n",
        "\n",
        "def generate_summary(file_name, top_n=5):\n",
        "    stop_words = stopwords.words('english')\n",
        "    summarize_text = []\n",
        "\n",
        "    # Step 1 - Read text anc split it\n",
        "    sentences =  read_article(file_name)\n",
        "\n",
        "    # Step 2 - Generate Similary Martix across sentences\n",
        "    sentence_similarity_martix = build_similarity_matrix(sentences, stop_words)\n",
        "\n",
        "    # Step 3 - Rank sentences in similarity martix\n",
        "    sentence_similarity_graph = nx.from_numpy_array(sentence_similarity_martix)\n",
        "    scores = nx.pagerank(sentence_similarity_graph)\n",
        "\n",
        "    # Step 4 - Sort the rank and pick top sentences\n",
        "    ranked_sentence = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)\n",
        "    print(ranked_sentence)\n",
        "\n",
        "    for i in range(top_n):\n",
        "      summarize_text.append(\" \".join(ranked_sentence[i][1]))\n",
        "\n",
        "    # Step 5 - Offcourse, output the summarize texr\n",
        "    print(\"Summarize Text: \\n\", \". \".join(summarize_text))\n",
        "\n",
        "# let's begin\n",
        "generate_summary( \"/content/companyInfo.txt\")"
      ]
    }
  ]
}